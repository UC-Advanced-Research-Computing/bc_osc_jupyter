---
cluster: "arcc2"
form:
  - mode
  - auto_accounts
  - custom_queue
  - bc_num_hours
  - bc_num_slots
  - num_cpus
  - num_gpus
  - custom_element
attributes:
  custom_queue:
    widget: "select"
    label: "Partition"
    cacheable: false
    options:
      - ["Public", "public"]
      - ["Short", "short"]
      - ["Large Memory", "large-mem"]
      - ["GPU V100", "gpu-v100"]
      - ["GPU A100", "gpu-a100"]
      - ["GPU H100", "gpu-h100"]
  bc_num_hours:
    label: Walltime (Hours)
    help: |
      - Partition durations:
        - Short: 4 hours maximum
        - All others: 480 hours maximum
      - Exceeding walltime time will automatically stop this job.
    cacheable: false
    widget: number_field
    max: 480
    min: 1
    step: 1
    value: 1
  bc_num_slots: 1
  mode:
    widget: "radio"
    value: "1"
    options:
      - ["Jupyter Lab", "1"]
      - ["Jupyter Notebook", "0"]
  num_cpus:
    label: CPUs (Cores)
    help: |
      - CPU Paritions: allocate up to 64 cores
      - GPU Partitions:
        - V100 and A100: allocate up to 32 CPU cores per GPU
        - H100: allocate up to 12 CPU cores per GPU
    cacheable: false
    widget: number_field
    max: 96
    min: 1
    step: 1
    value: 1
  num_gpus:
    label: "GPUs"
    help: |
       - GPUs Available:
           - V100 and A100: 2 GPUs per node
           - H100: 8 GPUs per node
       - If requesting GPUs, you must select a GPU-enabled partition above
    cacheable: false
    widget: number_field
    max: 8
    min: 0
    step: 1
    value: 0
  custom_element:
      class: "d-none"
      skip_label: true
      help: |
        <div class="card mb-3">
          <div class="card-header">
            Memory per CPU
          </div>
          <div class="card-body">
            <p class="mb-2">
              Total memory available ≈ <strong>CPUs x Mem/CPU</strong>.
            </p>
            <div class="table-responsive">
              <table class="table table-sm mb-0">
                <thead>
                  <tr><td>Partition</td><td>Mem / CPU</td></tr>
                </thead>
                <tbody>
                  <tr><td>Public</td><td>3800&nbsp;MB (≈ 3.71 GiB)</td></tr>
                  <tr><td>Short</td><td>3800&nbsp;MB (≈ 3.71 GiB)</td></tr>
                  <tr><td>Large Memory</td><td>30400&nbsp;MB (≈ 29.7 GiB)</td></tr>
                  <tr><td>GPU-V100</td><td>3800&nbsp;MB (≈ 3.71 GiB)</td></tr>
                  <tr><td>GPU-A100</td><td>15200&nbsp;MB (≈ 14.8 GiB)</td></tr>
                  <tr><td>GPU-H100</td><td>10125&nbsp;MB (≈ 9.89 GiB)</td></tr>
                </tbody>
              </table>
            </div>
          </div>
        </div>

        <div class="card mb-3">
          <div class="card-header">
            Note
          </div>
          <div class="card-body">
            Jupyter kernels must be setup for your python virtual environments and/or your conda environments to be available.
            If you need help with that setup, click the buttton below.
          </div>
        </div>

        <button class="btn btn-dark mt-3" type="button" data-toggle="collapse" data-target="#env_setup" aria-expanded="false" aria-controls="collapseExample">
          How to install Jupyter kernels
        </button>

        <ol class="collapse" id="env_setup">
          <li>Login to ARCC2 using your terminal of choice</li>
          <li>Activate your conda or virtual environment as you normally would</li>
          <li>Install the package <span class="bg-light text-black">ipykernel</span> into your environment
            <ul>
              <li>Conda Env: <span class="bg-light text-black">conda install ipykernel</span></li>
              <li>Python Virtual Env: <span class="bg-light text-black">pip install ipykernel</span></li>
            </ul>
          <li>Install the kernel; be sure to set the <span class="bg-light text-black">name</span> appropriately</li>
            <ul>
              <li><span class="bg-light text-black">python -m ipykernel install --user --name=ENV_NAME</span></li>
            </ul>
        </ol>
